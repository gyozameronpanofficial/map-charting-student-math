{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# MAP Competition - Advanced Baseline v2.0\n",
    "\n",
    "## Overview\n",
    "Robust implementation of improvement strategies:\n",
    "- **Strategy 2**: Mathematical Feature Engineering\n",
    "- **Strategy 4**: MAP@3 Optimization\n",
    "\n",
    "**Target**: Beat current #1 position (Public LB: 0.841)  \n",
    "**Expected improvement**: +0.025-0.065 MAP@3 score\n",
    "\n",
    "## Key Improvements\n",
    "- Robust error handling for all feature extraction\n",
    "- Safe mathematical feature computation\n",
    "- LightGBM instead of XGBoost for stability\n",
    "- Comprehensive text preprocessing\n",
    "- MAP@3 optimized prediction generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "imports",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK WordNetLemmatizer loaded successfully\n",
      "All imports completed successfully\n",
      "CPU times: user 2.4 s, sys: 403 ms, total: 2.81 s\n",
      "Wall time: 1.28 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "import lightgbm as lgb\n",
    "from scipy import sparse\n",
    "\n",
    "# Text Processing\n",
    "try:\n",
    "    import nltk\n",
    "    from nltk.stem import WordNetLemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    print(\"NLTK WordNetLemmatizer loaded successfully\")\n",
    "except:\n",
    "    lemmatizer = None\n",
    "    print(\"NLTK WordNetLemmatizer not available, using basic processing\")\n",
    "\n",
    "import time\n",
    "import os\n",
    "\n",
    "print(\"All imports completed successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data-loading",
   "metadata": {},
   "source": [
    "## 2. Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "load-data",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Train shape: (36696, 7)\n",
      "Test shape: (3, 5)\n",
      "Unique categories: 6\n",
      "Unique misconceptions: 36\n",
      "Unique combinations: 65\n",
      "\n",
      "Target distribution:\n",
      "Category\n",
      "True_Correct           14802\n",
      "False_Misconception     9457\n",
      "False_Neither           6542\n",
      "True_Neither            5265\n",
      "True_Misconception       403\n",
      "False_Correct            227\n",
      "Name: count, dtype: int64\n",
      "CPU times: user 61.8 ms, sys: 13 ms, total: 74.8 ms\n",
      "Wall time: 78.5 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Data paths - Local development\n",
    "train_path = \"/Users/osawa/kaggle/map-charting-student-math-misunderstandings/data/raw/train.csv\"\n",
    "test_path = \"/Users/osawa/kaggle/map-charting-student-math-misunderstandings/data/raw/test.csv\"\n",
    "sample_path = \"/Users/osawa/kaggle/map-charting-student-math-misunderstandings/data/raw/sample_submission.csv\"\n",
    "\n",
    "# For Kaggle submission, uncomment these lines:\n",
    "# train_path = \"/kaggle/input/map-charting-student-math-misunderstandings/train.csv\"\n",
    "# test_path = \"/kaggle/input/map-charting-student-math-misunderstandings/test.csv\"\n",
    "# sample_path = \"/kaggle/input/map-charting-student-math-misunderstandings/sample_submission.csv\"\n",
    "\n",
    "print(\"Loading data...\")\n",
    "train = pd.read_csv(train_path)\n",
    "test = pd.read_csv(test_path)\n",
    "sample_submission = pd.read_csv(sample_path)\n",
    "\n",
    "print(f\"Train shape: {train.shape}\")\n",
    "print(f\"Test shape: {test.shape}\")\n",
    "\n",
    "# Prepare targets\n",
    "train['Misconception'] = train['Misconception'].fillna('NA')\n",
    "train['target_combined'] = train['Category'] + ':' + train['Misconception']\n",
    "\n",
    "print(f\"Unique categories: {train['Category'].nunique()}\")\n",
    "print(f\"Unique misconceptions: {train['Misconception'].nunique()}\")\n",
    "print(f\"Unique combinations: {train['target_combined'].nunique()}\")\n",
    "\n",
    "print(\"\\nTarget distribution:\")\n",
    "print(train['Category'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feature-classes",
   "metadata": {},
   "source": [
    "## 3. Feature Extraction Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "math-features",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MathematicalFeatureExtractor defined\n"
     ]
    }
   ],
   "source": [
    "class MathematicalFeatureExtractor:\n",
    "    \"\"\"Extract mathematical features from text with robust error handling\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Mathematical patterns (fixed regex escaping)\n",
    "        self.fraction_pattern = re.compile(r'\\\\frac\\{([^}]+)\\}\\{([^}]+)\\}')\n",
    "        self.simple_fraction_pattern = re.compile(r'(\\d+)\\s*/\\s*(\\d+)')\n",
    "        self.decimal_pattern = re.compile(r'\\d+\\.\\d+')\n",
    "        self.percentage_pattern = re.compile(r'\\d+%')\n",
    "        self.number_pattern = re.compile(r'\\b\\d+\\b')\n",
    "        self.operation_pattern = re.compile(r'[+\\-*/=]')\n",
    "        \n",
    "        # Mathematical concepts\n",
    "        self.math_concepts = {\n",
    "            'fraction': ['fraction', 'numerator', 'denominator', 'over', 'divided'],\n",
    "            'decimal': ['decimal', 'point', 'place', 'tenths', 'hundredths'],\n",
    "            'percentage': ['percent', 'percentage', '%', 'out of 100'],\n",
    "            'addition': ['add', 'plus', 'sum', 'total', 'altogether'],\n",
    "            'subtraction': ['subtract', 'minus', 'difference', 'take away'],\n",
    "            'multiplication': ['multiply', 'times', 'product', 'of'],\n",
    "            'division': ['divide', 'quotient', 'split', 'share'],\n",
    "            'comparison': ['greater', 'less', 'equal', 'bigger', 'smaller', 'same']\n",
    "        }\n",
    "        \n",
    "    def safe_extract_numbers(self, text):\n",
    "        \"\"\"Safely extract numbers with bounds checking\"\"\"\n",
    "        try:\n",
    "            numbers = []\n",
    "            for match in self.number_pattern.findall(str(text)):\n",
    "                try:\n",
    "                    num = float(match)\n",
    "                    if 0 <= num <= 1e6:  # Reasonable bounds\n",
    "                        numbers.append(num)\n",
    "                except (ValueError, OverflowError):\n",
    "                    continue\n",
    "            return numbers\n",
    "        except:\n",
    "            return []\n",
    "    \n",
    "    def extract_numerical_features(self, text):\n",
    "        \"\"\"Extract numerical features safely\"\"\"\n",
    "        text = str(text)\n",
    "        features = {}\n",
    "        \n",
    "        # Count patterns\n",
    "        features['fraction_count'] = len(self.fraction_pattern.findall(text))\n",
    "        features['simple_fraction_count'] = len(self.simple_fraction_pattern.findall(text))\n",
    "        features['decimal_count'] = len(self.decimal_pattern.findall(text))\n",
    "        features['percentage_count'] = len(self.percentage_pattern.findall(text))\n",
    "        features['operation_count'] = len(self.operation_pattern.findall(text))\n",
    "        \n",
    "        # Number analysis\n",
    "        numbers = self.safe_extract_numbers(text)\n",
    "        features['number_count'] = len(numbers)\n",
    "        \n",
    "        if numbers:\n",
    "            features['max_number'] = max(numbers)\n",
    "            features['min_number'] = min(numbers)\n",
    "            features['number_range'] = features['max_number'] - features['min_number']\n",
    "            features['avg_number'] = np.mean(numbers)\n",
    "        else:\n",
    "            features['max_number'] = 0\n",
    "            features['min_number'] = 0\n",
    "            features['number_range'] = 0\n",
    "            features['avg_number'] = 0\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def extract_concept_features(self, text):\n",
    "        \"\"\"Extract mathematical concept features\"\"\"\n",
    "        text_lower = str(text).lower()\n",
    "        features = {}\n",
    "        \n",
    "        for concept, keywords in self.math_concepts.items():\n",
    "            count = sum(1 for kw in keywords if kw in text_lower)\n",
    "            features[f'{concept}_concept'] = min(count, 10)  # Cap at 10\n",
    "            features[f'has_{concept}'] = 1 if count > 0 else 0\n",
    "            \n",
    "        return features\n",
    "    \n",
    "    def extract_complexity_features(self, text):\n",
    "        \"\"\"Extract text complexity features\"\"\"\n",
    "        text = str(text)\n",
    "        features = {}\n",
    "        \n",
    "        # Basic text stats\n",
    "        features['text_length'] = min(len(text), 5000)\n",
    "        words = text.split()\n",
    "        features['word_count'] = len(words)\n",
    "        \n",
    "        if words:\n",
    "            features['avg_word_length'] = min(np.mean([len(w) for w in words]), 20)\n",
    "        else:\n",
    "            features['avg_word_length'] = 0\n",
    "            \n",
    "        features['sentence_count'] = min(len(re.split(r'[.!?]', text)), 50)\n",
    "        features['has_latex'] = 1 if '\\\\' in text else 0\n",
    "        features['parentheses_count'] = min(text.count('(') + text.count(')'), 20)\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def extract_all_features(self, text):\n",
    "        \"\"\"Extract all features with error handling\"\"\"\n",
    "        all_features = {}\n",
    "        \n",
    "        try:\n",
    "            all_features.update(self.extract_numerical_features(text))\n",
    "        except Exception:\n",
    "            pass\n",
    "            \n",
    "        try:\n",
    "            all_features.update(self.extract_concept_features(text))\n",
    "        except Exception:\n",
    "            pass\n",
    "            \n",
    "        try:\n",
    "            all_features.update(self.extract_complexity_features(text))\n",
    "        except Exception:\n",
    "            pass\n",
    "        \n",
    "        # Ensure all values are finite\n",
    "        for key, value in all_features.items():\n",
    "            if not np.isfinite(value):\n",
    "                all_features[key] = 0\n",
    "                \n",
    "        return all_features\n",
    "\n",
    "print(\"MathematicalFeatureExtractor defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "semantic-features",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SemanticFeatureExtractor defined\n"
     ]
    }
   ],
   "source": [
    "class SemanticFeatureExtractor:\n",
    "    \"\"\"Extract semantic relationship features\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def word_overlap_similarity(text1, text2):\n",
    "        \"\"\"Calculate word overlap similarity\"\"\"\n",
    "        try:\n",
    "            words1 = set(str(text1).lower().split())\n",
    "            words2 = set(str(text2).lower().split())\n",
    "            if not words1 or not words2:\n",
    "                return 0\n",
    "            intersection = len(words1.intersection(words2))\n",
    "            union = len(words1.union(words2))\n",
    "            return intersection / union if union > 0 else 0\n",
    "        except:\n",
    "            return 0\n",
    "    \n",
    "    def extract_features(self, df):\n",
    "        \"\"\"Extract semantic features from dataframe\"\"\"\n",
    "        features = {}\n",
    "        \n",
    "        # Similarity features\n",
    "        features['question_answer_similarity'] = df.apply(\n",
    "            lambda x: self.word_overlap_similarity(x['QuestionText'], x['MC_Answer']), axis=1\n",
    "        )\n",
    "        \n",
    "        features['question_explanation_similarity'] = df.apply(\n",
    "            lambda x: self.word_overlap_similarity(x['QuestionText'], x['StudentExplanation']), axis=1\n",
    "        )\n",
    "        \n",
    "        features['answer_explanation_similarity'] = df.apply(\n",
    "            lambda x: self.word_overlap_similarity(x['MC_Answer'], x['StudentExplanation']), axis=1\n",
    "        )\n",
    "        \n",
    "        # Length features\n",
    "        q_len = df['QuestionText'].str.len().fillna(1)\n",
    "        e_len = df['StudentExplanation'].str.len().fillna(0)\n",
    "        \n",
    "        features['explanation_question_length_ratio'] = np.clip(e_len / q_len, 0, 10)\n",
    "        features['explanation_length'] = np.clip(e_len, 0, 1000)\n",
    "        features['question_length'] = np.clip(q_len, 0, 1000)\n",
    "        \n",
    "        return pd.DataFrame(features)\n",
    "\n",
    "print(\"SemanticFeatureExtractor defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "text-processor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TextProcessor defined\n"
     ]
    }
   ],
   "source": [
    "class TextProcessor:\n",
    "    \"\"\"Handle text preprocessing\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.lemmatizer = lemmatizer\n",
    "    \n",
    "    def clean_text(self, text):\n",
    "        \"\"\"Clean text for processing\"\"\"\n",
    "        text = str(text)\n",
    "        \n",
    "        # Basic cleaning\n",
    "        text = re.sub(r'\\n+', ' ', text)\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        text = text.strip().lower()\n",
    "        \n",
    "        # Keep alphanumeric, basic math, and spaces\n",
    "        text = re.sub(r'[^a-zA-Z0-9\\s+\\-*/=().]', ' ', text)\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def lemmatize_text(self, text):\n",
    "        \"\"\"Lemmatize text if lemmatizer available\"\"\"\n",
    "        if self.lemmatizer:\n",
    "            try:\n",
    "                words = text.split()\n",
    "                lemmatized = [self.lemmatizer.lemmatize(word) for word in words]\n",
    "                return ' '.join(lemmatized)\n",
    "            except:\n",
    "                return text\n",
    "        return text\n",
    "    \n",
    "    def process_text(self, text):\n",
    "        \"\"\"Full text processing pipeline\"\"\"\n",
    "        text = self.clean_text(text)\n",
    "        text = self.lemmatize_text(text)\n",
    "        return text\n",
    "\n",
    "print(\"TextProcessor defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "map3-optimizer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP3Optimizer defined\n"
     ]
    }
   ],
   "source": [
    "class MAP3Optimizer:\n",
    "    \"\"\"MAP@3 specific optimization and evaluation\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def map3_score(y_true, y_pred_proba, class_names):\n",
    "        \"\"\"Calculate MAP@3 score\"\"\"\n",
    "        scores = []\n",
    "        \n",
    "        for i, true_label in enumerate(y_true):\n",
    "            # Get top 3 predictions\n",
    "            top_3_indices = np.argsort(y_pred_proba[i])[::-1][:3]\n",
    "            \n",
    "            # Find rank of true label\n",
    "            score = 0.0\n",
    "            for rank, pred_idx in enumerate(top_3_indices, 1):\n",
    "                pred_label = class_names[pred_idx]\n",
    "                if pred_label == true_label:\n",
    "                    score = 1.0 / rank\n",
    "                    break\n",
    "            \n",
    "            scores.append(score)\n",
    "        \n",
    "        return np.mean(scores)\n",
    "    \n",
    "    @staticmethod\n",
    "    def generate_combined_predictions(cat_probs, misc_probs, category_classes, misconception_classes, top_k=3):\n",
    "        \"\"\"Generate combined Category:Misconception predictions\"\"\"\n",
    "        predictions = []\n",
    "        \n",
    "        for i in range(len(cat_probs)):\n",
    "            pred_combos = []\n",
    "            \n",
    "            # Get top categories and misconceptions\n",
    "            top_cats = np.argsort(cat_probs[i])[::-1][:top_k]\n",
    "            top_miscs = np.argsort(misc_probs[i])[::-1][:top_k]\n",
    "            \n",
    "            # Generate combinations\n",
    "            for cat_idx in top_cats:\n",
    "                cat_name = category_classes[cat_idx]\n",
    "                cat_prob = cat_probs[i][cat_idx]\n",
    "                \n",
    "                if 'Misconception' in cat_name:\n",
    "                    # Add top misconceptions for misconception categories\n",
    "                    for misc_idx in top_miscs:\n",
    "                        misc_name = misconception_classes[misc_idx]\n",
    "                        misc_prob = misc_probs[i][misc_idx]\n",
    "                        \n",
    "                        if misc_name != 'NA':\n",
    "                            combined_label = f\"{cat_name}:{misc_name}\"\n",
    "                            combined_prob = cat_prob * misc_prob\n",
    "                            pred_combos.append((combined_label, combined_prob))\n",
    "                else:\n",
    "                    # Non-misconception categories always use NA\n",
    "                    combined_label = f\"{cat_name}:NA\"\n",
    "                    pred_combos.append((combined_label, cat_prob))\n",
    "            \n",
    "            # Sort by probability and take top 3\n",
    "            pred_combos.sort(key=lambda x: x[1], reverse=True)\n",
    "            top_3 = [combo[0] for combo in pred_combos[:3]]\n",
    "            \n",
    "            # Ensure exactly 3 predictions\n",
    "            while len(top_3) < 3:\n",
    "                top_3.append(\"True_Correct:NA\")\n",
    "            \n",
    "            predictions.append(top_3)\n",
    "        \n",
    "        return predictions\n",
    "\n",
    "print(\"MAP3Optimizer defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "text-processing",
   "metadata": {},
   "source": [
    "## 4. Text Processing and Feature Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "create-text",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating combined text...\n",
      "Processing text...\n",
      "Text processing completed\n",
      "Sample processed text: question what fraction of the shape is not shaded give your answer in its simplest form. image a triangle split into 9 equal smaller triangles. 6 of them are shaded. answer ( frac 1 3 ) explanation 0n...\n",
      "CPU times: user 3.86 s, sys: 1.12 s, total: 4.98 s\n",
      "Wall time: 5.29 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "print(\"Creating combined text...\")\n",
    "train['combined_text'] = (\"Question: \" + train['QuestionText'].astype(str) + \n",
    "                         \" Answer: \" + train['MC_Answer'].astype(str) + \n",
    "                         \" Explanation: \" + train['StudentExplanation'].astype(str))\n",
    "\n",
    "test['combined_text'] = (\"Question: \" + test['QuestionText'].astype(str) + \n",
    "                        \" Answer: \" + test['MC_Answer'].astype(str) + \n",
    "                        \" Explanation: \" + test['StudentExplanation'].astype(str))\n",
    "\n",
    "print(\"Processing text...\")\n",
    "processor = TextProcessor()\n",
    "train['processed_text'] = train['combined_text'].apply(processor.process_text)\n",
    "test['processed_text'] = test['combined_text'].apply(processor.process_text)\n",
    "\n",
    "print(\"Text processing completed\")\n",
    "print(f\"Sample processed text: {train['processed_text'].iloc[0][:200]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "extract-math",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting mathematical features...\n",
      "  Processing 0/36696\n",
      "  Processing 5000/36696\n",
      "  Processing 10000/36696\n",
      "  Processing 15000/36696\n",
      "  Processing 20000/36696\n",
      "  Processing 25000/36696\n",
      "  Processing 30000/36696\n",
      "  Processing 35000/36696\n",
      "Mathematical features shape: (36696, 32)\n",
      "Sample features: ['fraction_count', 'simple_fraction_count', 'decimal_count', 'percentage_count', 'operation_count', 'number_count', 'max_number', 'min_number', 'number_range', 'avg_number']\n",
      "CPU times: user 2.07 s, sys: 53.6 ms, total: 2.12 s\n",
      "Wall time: 2.23 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "print(\"Extracting mathematical features...\")\n",
    "math_extractor = MathematicalFeatureExtractor()\n",
    "\n",
    "train_math_features = []\n",
    "for i, text in enumerate(train['combined_text']):\n",
    "    if i % 5000 == 0:\n",
    "        print(f\"  Processing {i}/{len(train)}\")\n",
    "    features = math_extractor.extract_all_features(text)\n",
    "    train_math_features.append(features)\n",
    "\n",
    "test_math_features = []\n",
    "for text in test['combined_text']:\n",
    "    features = math_extractor.extract_all_features(text)\n",
    "    test_math_features.append(features)\n",
    "\n",
    "train_math_df = pd.DataFrame(train_math_features).fillna(0)\n",
    "test_math_df = pd.DataFrame(test_math_features).fillna(0)\n",
    "\n",
    "print(f\"Mathematical features shape: {train_math_df.shape}\")\n",
    "print(f\"Sample features: {list(train_math_df.columns)[:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "extract-semantic",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting semantic features...\n",
      "Semantic features shape: (36696, 6)\n",
      "Semantic features: ['question_answer_similarity', 'question_explanation_similarity', 'answer_explanation_similarity', 'explanation_question_length_ratio', 'explanation_length', 'question_length']\n",
      "CPU times: user 732 ms, sys: 14.5 ms, total: 747 ms\n",
      "Wall time: 789 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "print(\"Extracting semantic features...\")\n",
    "semantic_extractor = SemanticFeatureExtractor()\n",
    "train_semantic_df = semantic_extractor.extract_features(train)\n",
    "test_semantic_df = semantic_extractor.extract_features(test)\n",
    "\n",
    "print(f\"Semantic features shape: {train_semantic_df.shape}\")\n",
    "print(f\"Semantic features: {list(train_semantic_df.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "create-tfidf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating TF-IDF features...\n",
      "TF-IDF shape: (36696, 20000)\n",
      "CPU times: user 1.76 s, sys: 40.8 ms, total: 1.8 s\n",
      "Wall time: 1.87 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "print(\"Creating TF-IDF features...\")\n",
    "tfidf = TfidfVectorizer(\n",
    "    stop_words='english',\n",
    "    ngram_range=(1, 3),\n",
    "    max_df=0.95,\n",
    "    min_df=2,\n",
    "    max_features=20000\n",
    ")\n",
    "\n",
    "all_text = pd.concat([train['processed_text'], test['processed_text']])\n",
    "tfidf.fit(all_text)\n",
    "\n",
    "train_tfidf = tfidf.transform(train['processed_text'])\n",
    "test_tfidf = tfidf.transform(test['processed_text'])\n",
    "\n",
    "print(f\"TF-IDF shape: {train_tfidf.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "combine-features",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combining features...\n",
      "Combined features shape: (36696, 20038)\n",
      "Feature breakdown:\n",
      "  TF-IDF: 20000\n",
      "  Mathematical: 32\n",
      "  Semantic: 6\n",
      "  Total: 20038\n",
      "CPU times: user 17.6 ms, sys: 3.89 ms, total: 21.4 ms\n",
      "Wall time: 20.5 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "print(\"Combining features...\")\n",
    "train_math_sparse = sparse.csr_matrix(train_math_df.values)\n",
    "test_math_sparse = sparse.csr_matrix(test_math_df.values)\n",
    "\n",
    "train_semantic_sparse = sparse.csr_matrix(train_semantic_df.values)\n",
    "test_semantic_sparse = sparse.csr_matrix(test_semantic_df.values)\n",
    "\n",
    "train_features = sparse.hstack([train_tfidf, train_math_sparse, train_semantic_sparse])\n",
    "test_features = sparse.hstack([test_tfidf, test_math_sparse, test_semantic_sparse])\n",
    "\n",
    "print(f\"Combined features shape: {train_features.shape}\")\n",
    "print(f\"Feature breakdown:\")\n",
    "print(f\"  TF-IDF: {train_tfidf.shape[1]}\")\n",
    "print(f\"  Mathematical: {train_math_df.shape[1]}\")\n",
    "print(f\"  Semantic: {train_semantic_df.shape[1]}\")\n",
    "print(f\"  Total: {train_features.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model-training",
   "metadata": {},
   "source": [
    "## 5. Model Training and Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "prepare-targets",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categories (6): ['False_Correct', 'False_Misconception', 'False_Neither', 'True_Correct', 'True_Misconception', 'True_Neither']\n",
      "Misconceptions (36): ['Adding_across', 'Adding_terms', 'Additive', 'Base_rate', 'Certainty', 'Definition', 'Denominator-only_change', 'Division', 'Duplication', 'Firstterm']...\n"
     ]
    }
   ],
   "source": [
    "# Prepare targets\n",
    "categories = sorted(train['Category'].unique())\n",
    "misconceptions = sorted(train['Misconception'].unique())\n",
    "\n",
    "cat_to_idx = {cat: idx for idx, cat in enumerate(categories)}\n",
    "misc_to_idx = {misc: idx for idx, misc in enumerate(misconceptions)}\n",
    "\n",
    "train['cat_target'] = train['Category'].map(cat_to_idx)\n",
    "train['misc_target'] = train['Misconception'].map(misc_to_idx)\n",
    "\n",
    "print(f\"Categories ({len(categories)}): {categories}\")\n",
    "print(f\"Misconceptions ({len(misconceptions)}): {misconceptions[:10]}...\")  # Show first 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cross-validation",
   "metadata": {},
   "outputs": [],
   "source": "%%time\n\nprint(\"Starting cross-validation training...\")\nn_folds = 5  # Reduced for faster execution\nskf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=42)\n\noof_cat_preds = np.zeros((len(train), len(categories)))\noof_misc_preds = np.zeros((len(train), len(misconceptions)))\n\ntest_cat_preds = np.zeros((len(test), len(categories)))\ntest_misc_preds = np.zeros((len(test), len(misconceptions)))\n\nfold_map3_scores = []\n\ndef calculate_map3(true_labels, predictions):\n    \"\"\"Calculate MAP@3 score for validation\"\"\"\n    scores = []\n    for true_label, pred_list in zip(true_labels, predictions):\n        score = 0.0\n        for rank, pred in enumerate(pred_list, 1):\n            if pred == true_label:\n                score = 1.0 / rank\n                break\n        scores.append(score)\n    return np.mean(scores)\n\nfor fold, (train_idx, val_idx) in enumerate(skf.split(train_features, train['cat_target'])):\n    print(f\"\\nFold {fold + 1}/{n_folds}\")\n    print(f\"  Train size: {len(train_idx)}, Val size: {len(val_idx)}\")\n    \n    # Split data\n    X_train, X_val = train_features[train_idx], train_features[val_idx]\n    y_cat_train, y_cat_val = train['cat_target'].iloc[train_idx], train['cat_target'].iloc[val_idx]\n    y_misc_train, y_misc_val = train['misc_target'].iloc[train_idx], train['misc_target'].iloc[val_idx]\n    \n    # Train category model\n    print(\"  Training category model...\")\n    cat_model = LogisticRegression(max_iter=1000, random_state=42, C=1.0)\n    cat_model.fit(X_train, y_cat_train)\n    \n    # Train misconception model  \n    print(\"  Training misconception model...\")\n    misc_model = lgb.LGBMClassifier(\n        n_estimators=100,\n        max_depth=6,\n        learning_rate=0.1,\n        random_state=42,\n        objective='multiclass',\n        metric='multi_logloss',\n        verbosity=-1\n    )\n    misc_model.fit(X_train, y_misc_train)\n    \n    # Predictions\n    print(\"  Making predictions...\")\n    oof_cat_preds[val_idx] = cat_model.predict_proba(X_val)\n    oof_misc_preds[val_idx] = misc_model.predict_proba(X_val)\n    \n    test_cat_preds += cat_model.predict_proba(test_features) / n_folds\n    test_misc_preds += misc_model.predict_proba(test_features) / n_folds\n    \n    # Calculate MAP@3 for this fold (PROPER EVALUATION)\n    print(\"  Calculating MAP@3...\")\n    val_predictions = MAP3Optimizer.generate_combined_predictions(\n        oof_cat_preds[val_idx], oof_misc_preds[val_idx], \n        categories, misconceptions\n    )\n    \n    val_true = train['target_combined'].iloc[val_idx].tolist()\n    \n    # Calculate proper MAP@3 score\n    fold_map3 = calculate_map3(val_true, val_predictions)\n    fold_map3_scores.append(fold_map3)\n    \n    # Also calculate top-1 accuracy for reference\n    val_pred_first = [pred[0] for pred in val_predictions]\n    fold_acc = np.mean([true == pred for true, pred in zip(val_true, val_pred_first)])\n    \n    print(f\"  Fold {fold + 1} MAP@3: {fold_map3:.6f}\")\n    print(f\"  Fold {fold + 1} Top-1 Acc: {fold_acc:.6f}\")\n\nprint(f\"\\nüéØ CROSS-VALIDATION RESULTS (MAP@3):\")\nprint(f\"   Mean MAP@3: {np.mean(fold_map3_scores):.6f}\")\nprint(f\"   Std MAP@3:  {np.std(fold_map3_scores):.6f}\")\nprint(f\"   CV Stability: {np.std(fold_map3_scores)/np.mean(fold_map3_scores)*100:.2f}%\")\n\ncv_map3 = np.mean(fold_map3_scores)\nprint(f\"\\nüìä EXPECTED PERFORMANCE:\")\nprint(f\"   Public LB (estimated): {cv_map3:.6f}\")\nprint(f\"   vs Current #1 (0.841): {cv_map3 - 0.841:+.6f} ({'BEAT' if cv_map3 > 0.841 else 'MISS'})\")\nprint(f\"   vs Fork baseline (0.852): {cv_map3 - 0.852:+.6f} ({'BEAT' if cv_map3 > 0.852 else 'MISS'})\")\n\nprint(\"Cross-validation completed successfully\")"
  },
  {
   "cell_type": "markdown",
   "id": "evaluation",
   "metadata": {},
   "source": [
    "## 6. Final Evaluation and Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "final-evaluation",
   "metadata": {},
   "outputs": [],
   "source": "%%time\n\nprint(\"Calculating overall out-of-fold performance...\")\n\n# Generate OOF predictions using all CV predictions\noof_predictions = MAP3Optimizer.generate_combined_predictions(\n    oof_cat_preds, oof_misc_preds, categories, misconceptions\n)\n\n# Calculate final MAP@3 on entire training set\ntrue_labels = train['target_combined'].tolist()\noverall_map3 = calculate_map3(true_labels, oof_predictions)\n\n# Calculate accuracies for each position (for additional insights)\ntop1_acc = np.mean([true == pred[0] for true, pred in zip(true_labels, oof_predictions)])\ntop2_acc = np.mean([true in pred[:2] for true, pred in zip(true_labels, oof_predictions)])\ntop3_acc = np.mean([true in pred[:3] for true, pred in zip(true_labels, oof_predictions)])\n\nprint(f\"\\nüìà FINAL OUT-OF-FOLD PERFORMANCE:\")\nprint(f\"   MAP@3 Score: {overall_map3:.6f}\")\nprint(f\"   Top-1 Accuracy: {top1_acc:.6f}\")\nprint(f\"   Top-2 Accuracy: {top2_acc:.6f}\")\nprint(f\"   Top-3 Accuracy: {top3_acc:.6f}\")\n\nprint(f\"\\nüéØ CONSISTENCY CHECK:\")\nprint(f\"   CV MAP@3: {cv_map3:.6f}\")\nprint(f\"   OOF MAP@3: {overall_map3:.6f}\")\nprint(f\"   Difference: {abs(cv_map3 - overall_map3):.6f} ({'GOOD' if abs(cv_map3 - overall_map3) < 0.005 else 'CHECK'})\")\n\nprint(f\"\\nüèÜ COMPETITION BENCHMARKS:\")\nimprovement_vs_leader = overall_map3 - 0.841\nimprovement_vs_fork = overall_map3 - 0.852\nprint(f\"   Current #1 (0.841): {improvement_vs_leader:+.6f} ({'‚úÖ BEAT' if improvement_vs_leader > 0 else '‚ùå MISS'})\")\nprint(f\"   Fork baseline (0.852): {improvement_vs_fork:+.6f} ({'‚úÖ BEAT' if improvement_vs_fork > 0 else '‚ùå MISS'})\")\n\nif overall_map3 > 0.841:\n    print(f\"\\nüéâ SUCCESS! Expected to beat current #1 position!\")\n    if overall_map3 > 0.852:\n        print(f\"üöÄ EXCELLENT! Expected to beat even the fork baseline!\")\nelse:\n    print(f\"\\n‚ö†Ô∏è  Need improvement to beat current #1 position\")\n    print(f\"   Gap to close: {0.841 - overall_map3:.6f}\")\n\nprint(f\"\\nüìä VALIDATION SUMMARY:\")\nprint(f\"   Metric: MAP@3 (competition metric)\")\nprint(f\"   CV Method: {n_folds}-fold StratifiedKFold\")\nprint(f\"   Stability: {np.std(fold_map3_scores)/np.mean(fold_map3_scores)*100:.2f}% CV\")\nprint(f\"   Confidence: {'HIGH' if np.std(fold_map3_scores) < 0.01 else 'MEDIUM' if np.std(fold_map3_scores) < 0.02 else 'LOW'}\")"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "test-predictions",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating test predictions...\n",
      "Submission created with 3 rows\n",
      "\n",
      "Sample predictions:\n",
      "  Test 0: True_Correct:NA False_Neither:NA False_Misconception:Adding_across\n",
      "  Test 1: False_Neither:NA True_Correct:NA False_Misconception:Incomplete\n",
      "  Test 2: True_Neither:NA True_Correct:NA False_Misconception:Irrelevant\n",
      "\n",
      "Submission file:\n",
      "   row_id                             Category:Misconception\n",
      "0   36696  True_Correct:NA False_Neither:NA False_Misconc...\n",
      "1   36697  False_Neither:NA True_Correct:NA False_Misconc...\n",
      "2   36698  True_Neither:NA True_Correct:NA False_Misconce...\n",
      "CPU times: user 2.25 ms, sys: 1.06 ms, total: 3.3 ms\n",
      "Wall time: 3.28 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "print(\"Generating test predictions...\")\n",
    "test_predictions = MAP3Optimizer.generate_combined_predictions(\n",
    "    test_cat_preds, test_misc_preds, categories, misconceptions\n",
    ")\n",
    "\n",
    "# Create submission\n",
    "submission_data = []\n",
    "for i, preds in enumerate(test_predictions):\n",
    "    row_id = test.iloc[i]['row_id']\n",
    "    pred_str = ' '.join(preds)\n",
    "    submission_data.append({'row_id': row_id, 'Category:Misconception': pred_str})\n",
    "\n",
    "submission_df = pd.DataFrame(submission_data)\n",
    "submission_df.to_csv('submission.csv', index=False)\n",
    "\n",
    "print(f\"Submission created with {len(submission_df)} rows\")\n",
    "print(\"\\nSample predictions:\")\n",
    "for i in range(min(3, len(test_predictions))):\n",
    "    print(f\"  Test {i}: {' '.join(test_predictions[i])}\")\n",
    "\n",
    "print(\"\\nSubmission file:\")\n",
    "print(submission_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "analysis",
   "metadata": {},
   "source": [
    "## 7. Feature Analysis and Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "feature-analysis",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Feature Analysis ===\n",
      "\n",
      "Top Mathematical Features (by variance):\n",
      "number_range         3.218397e+07\n",
      "max_number           3.218391e+07\n",
      "avg_number           8.131850e+05\n",
      "text_length          5.906509e+03\n",
      "word_count           2.267055e+02\n",
      "min_number           1.867986e+02\n",
      "number_count         1.522297e+01\n",
      "operation_count      5.163082e+00\n",
      "parentheses_count    4.087267e+00\n",
      "sentence_count       3.208842e+00\n",
      "dtype: float64\n",
      "\n",
      "Semantic Feature Statistics:\n",
      "       question_answer_similarity  question_explanation_similarity  \\\n",
      "count                36696.000000                     36696.000000   \n",
      "mean                     0.144662                         0.059529   \n",
      "std                      0.140718                         0.056860   \n",
      "min                      0.000000                         0.000000   \n",
      "25%                      0.000000                         0.000000   \n",
      "50%                      0.100000                         0.052632   \n",
      "75%                      0.181818                         0.090909   \n",
      "max                      0.666667                         0.454545   \n",
      "\n",
      "       answer_explanation_similarity  explanation_question_length_ratio  \\\n",
      "count                   36696.000000                       36696.000000   \n",
      "mean                        0.016657                           1.205994   \n",
      "std                         0.032627                           1.160155   \n",
      "min                         0.000000                           0.014286   \n",
      "25%                         0.000000                           0.401575   \n",
      "50%                         0.000000                           0.821429   \n",
      "75%                         0.000000                           1.588235   \n",
      "max                         0.500000                          10.000000   \n",
      "\n",
      "       explanation_length  question_length  \n",
      "count        36696.000000     36696.000000  \n",
      "mean            69.953428        97.318427  \n",
      "std             38.658304        66.181566  \n",
      "min              1.000000        29.000000  \n",
      "25%             43.000000        36.000000  \n",
      "50%             60.000000        62.000000  \n",
      "75%             86.000000       160.000000  \n",
      "max            586.000000       226.000000  \n",
      "\n",
      "Category Prediction Confidence:\n",
      "Mean: 0.453, Std: 0.150\n",
      "\n",
      "Misconception Prediction Confidence:\n",
      "Mean: 1.000, Std: 0.002\n",
      "\n",
      "Accuracy by Category:\n",
      "  False_Correct: 0.000 (n=227)\n",
      "  False_Misconception: 0.020 (n=9457)\n",
      "  False_Neither: 0.399 (n=6542)\n",
      "  True_Correct: 0.887 (n=14802)\n",
      "  True_Misconception: 0.000 (n=403)\n",
      "  True_Neither: 0.152 (n=5265)\n"
     ]
    }
   ],
   "source": [
    "print(\"=== Feature Analysis ===\")\n",
    "\n",
    "# Mathematical features analysis\n",
    "print(\"\\nTop Mathematical Features (by variance):\")\n",
    "math_variance = train_math_df.var().sort_values(ascending=False)\n",
    "print(math_variance.head(10))\n",
    "\n",
    "# Semantic features analysis\n",
    "print(\"\\nSemantic Feature Statistics:\")\n",
    "print(train_semantic_df.describe())\n",
    "\n",
    "# Category distribution analysis\n",
    "print(\"\\nCategory Prediction Confidence:\")\n",
    "cat_confidence = np.max(oof_cat_preds, axis=1)\n",
    "print(f\"Mean: {cat_confidence.mean():.3f}, Std: {cat_confidence.std():.3f}\")\n",
    "\n",
    "print(\"\\nMisconception Prediction Confidence:\")\n",
    "misc_confidence = np.max(oof_misc_preds, axis=1)\n",
    "print(f\"Mean: {misc_confidence.mean():.3f}, Std: {misc_confidence.std():.3f}\")\n",
    "\n",
    "# Error analysis by category\n",
    "print(\"\\nAccuracy by Category:\")\n",
    "for cat in categories:\n",
    "    mask = train['Category'] == cat\n",
    "    if mask.sum() > 0:\n",
    "        cat_true = train.loc[mask, 'target_combined'].tolist()\n",
    "        cat_pred = [oof_predictions[i][0] for i in range(len(oof_predictions)) if mask.iloc[i]]\n",
    "        cat_acc = np.mean([t == p for t, p in zip(cat_true, cat_pred)])\n",
    "        print(f\"  {cat}: {cat_acc:.3f} (n={mask.sum()})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": "print(\"=\"*60)\nprint(\"=== ADVANCED BASELINE V2.0 SUMMARY ===\")\nprint(\"=\"*60)\n\nprint(f\"\\nüìä PERFORMANCE METRICS (MAP@3):\")\nprint(f\"   Cross-Validation: {cv_map3:.6f} ¬± {np.std(fold_map3_scores):.6f}\")\nprint(f\"   Out-of-Fold: {overall_map3:.6f}\")\nprint(f\"   CV Stability: {np.std(fold_map3_scores)/np.mean(fold_map3_scores)*100:.2f}%\")\n\nprint(f\"\\nüéØ COMPETITION TARGETS:\")\nimprovement_vs_leader = cv_map3 - 0.841\nimprovement_vs_fork = cv_map3 - 0.852\nstatus_leader = \"‚úÖ ACHIEVED\" if improvement_vs_leader > 0 else \"‚ùå MISSED\"\nstatus_fork = \"‚úÖ ACHIEVED\" if improvement_vs_fork > 0 else \"‚ùå MISSED\"\nprint(f\"   vs Current #1 (0.841): {improvement_vs_leader:+.6f} ({status_leader})\")\nprint(f\"   vs Fork baseline (0.852): {improvement_vs_fork:+.6f} ({status_fork})\")\n\nprint(f\"\\nüîß FEATURES IMPLEMENTED:\")\nprint(f\"   ‚úÖ Mathematical Feature Engineering ({train_math_df.shape[1]} features)\")\nprint(f\"   ‚úÖ Semantic Relationship Features ({train_semantic_df.shape[1]} features)\")\nprint(f\"   ‚úÖ Enhanced Text Processing\")\nprint(f\"   ‚úÖ MAP@3 Optimized Predictions\")\nprint(f\"   ‚úÖ PROPER MAP@3 Cross-Validation ‚≠ê\")\nprint(f\"   ‚úÖ Robust Error Handling\")\nprint(f\"   ‚úÖ LightGBM for Stability\")\n\nprint(f\"\\nüìà NEXT IMPROVEMENT OPPORTUNITIES:\")\nif cv_map3 <= 0.841:\n    print(f\"   üî• PRIORITY: Close gap of {0.841 - cv_map3:.6f} to beat #1\")\n    print(f\"   üéØ Strategy 1: Advanced Transformer Models\")\n    print(f\"   üéØ Strategy 3: Ensemble Methods\")\n    print(f\"   üéØ Feature Engineering Iteration\")\nelif cv_map3 <= 0.852:\n    print(f\"   üéØ Goal: Beat fork baseline (gap: {0.852 - cv_map3:.6f})\")\n    print(f\"   üîÆ Strategy 1: Advanced Transformer Models\")\n    print(f\"   üîÆ Strategy 3: Sophisticated Ensemble\")\nelse:\n    print(f\"   üöÄ Excellent baseline! Focus on:\")\n    print(f\"   üîÆ Ensemble diversity\")\n    print(f\"   üîÆ Model stability\")\n    print(f\"   üîÆ Overfitting prevention\")\n\nprint(f\"\\nüìÅ FILES CREATED:\")\nprint(f\"   üìÑ submission.csv (ready for Kaggle submission)\")\nprint(f\"   üìä Validation MAP@3: {cv_map3:.6f}\")\n\nprint(f\"\\nüéØ VALIDATION CONFIDENCE:\")\ncv_std = np.std(fold_map3_scores)\nif cv_std < 0.01:\n    confidence = \"üü¢ HIGH\"\nelif cv_std < 0.02:\n    confidence = \"üü° MEDIUM\" \nelse:\n    confidence = \"üî¥ LOW\"\nprint(f\"   {confidence} (CV std: {cv_std:.6f})\")\n\nprint(f\"\\n\" + \"=\"*60)\nif cv_map3 > 0.841:\n    print(\"üéâ READY FOR SUBMISSION - EXPECTED TO BEAT #1! üöÄ\")\nelse:\n    print(\"üìà GOOD BASELINE - NEEDS FURTHER IMPROVEMENT\")\nprint(\"=\"*60)"
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "summary",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "=== ADVANCED BASELINE V2.0 SUMMARY ===\n",
      "============================================================\n",
      "\n",
      "üìä PERFORMANCE METRICS:\n",
      "   MAP@3 Score: 0.544446\n",
      "   Top-1 Accuracy: 0.455717\n",
      "   CV Stability: 0.005700\n",
      "\n",
      "üéØ COMPETITION TARGETS:\n",
      "   vs Current #1 (0.841): -0.296554 (MISSED)\n",
      "   vs Fork baseline (0.852): -0.307554 (MISSED)\n",
      "\n",
      "üîß FEATURES IMPLEMENTED:\n",
      "   ‚úÖ Mathematical Feature Engineering (32 features)\n",
      "   ‚úÖ Semantic Relationship Features (6 features)\n",
      "   ‚úÖ Enhanced Text Processing\n",
      "   ‚úÖ MAP@3 Optimized Predictions\n",
      "   ‚úÖ Robust Error Handling\n",
      "   ‚úÖ LightGBM for Stability\n",
      "\n",
      "üìà NEXT IMPROVEMENT OPPORTUNITIES:\n",
      "   üîÆ Strategy 1: Advanced Transformer Models (DeBERTa/MathBERT)\n",
      "   üîÆ Strategy 3: Sophisticated Ensemble Methods\n",
      "   üîÆ Strategy 5: Data Augmentation & External Data\n",
      "   üîÆ Hyperparameter Optimization\n",
      "   üîÆ Feature Selection & Engineering\n",
      "\n",
      "üìÅ FILES CREATED:\n",
      "   üìÑ submission.csv (ready for Kaggle submission)\n",
      "\n",
      "============================================================\n",
      "Ready for submission to Kaggle! üöÄ\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"=== ADVANCED BASELINE V2.0 SUMMARY ===\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\nüìä PERFORMANCE METRICS:\")\n",
    "print(f\"   MAP@3 Score: {overall_map3:.6f}\")\n",
    "print(f\"   Top-1 Accuracy: {top1_acc:.6f}\")\n",
    "print(f\"   CV Stability: {np.std(fold_scores):.6f}\")\n",
    "\n",
    "print(f\"\\nüéØ COMPETITION TARGETS:\")\n",
    "improvement_vs_leader = overall_map3 - 0.841\n",
    "improvement_vs_fork = overall_map3 - 0.852\n",
    "print(f\"   vs Current #1 (0.841): {improvement_vs_leader:+.6f} ({'ACHIEVED' if improvement_vs_leader > 0 else 'MISSED'})\")\n",
    "print(f\"   vs Fork baseline (0.852): {improvement_vs_fork:+.6f} ({'ACHIEVED' if improvement_vs_fork > 0 else 'MISSED'})\")\n",
    "\n",
    "print(f\"\\nüîß FEATURES IMPLEMENTED:\")\n",
    "print(f\"   ‚úÖ Mathematical Feature Engineering ({train_math_df.shape[1]} features)\")\n",
    "print(f\"   ‚úÖ Semantic Relationship Features ({train_semantic_df.shape[1]} features)\")\n",
    "print(f\"   ‚úÖ Enhanced Text Processing\")\n",
    "print(f\"   ‚úÖ MAP@3 Optimized Predictions\")\n",
    "print(f\"   ‚úÖ Robust Error Handling\")\n",
    "print(f\"   ‚úÖ LightGBM for Stability\")\n",
    "\n",
    "print(f\"\\nüìà NEXT IMPROVEMENT OPPORTUNITIES:\")\n",
    "print(f\"   üîÆ Strategy 1: Advanced Transformer Models (DeBERTa/MathBERT)\")\n",
    "print(f\"   üîÆ Strategy 3: Sophisticated Ensemble Methods\")\n",
    "print(f\"   üîÆ Strategy 5: Data Augmentation & External Data\")\n",
    "print(f\"   üîÆ Hyperparameter Optimization\")\n",
    "print(f\"   üîÆ Feature Selection & Engineering\")\n",
    "\n",
    "print(f\"\\nüìÅ FILES CREATED:\")\n",
    "print(f\"   üìÑ submission.csv (ready for Kaggle submission)\")\n",
    "\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(\"Ready for submission to Kaggle! üöÄ\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bfc2f44",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}